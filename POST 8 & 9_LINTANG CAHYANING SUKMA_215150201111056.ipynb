{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jvonXDMsQP1k"},"source":["# Backpropagation"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import time"],"metadata":{"id":"YzH-ilE2twTF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pada post-test kali ini akan membandingkan dua jenis fungsi aktivasi yang biasa digunakan dalam backpropogation"],"metadata":{"id":"-gGbt71BdLJV"}},{"cell_type":"code","source":["#Fungsi Aktivasi Sigmoid dengan turunannya\n","def sig(x):\n","  return 1 / (1 + np.exp(-x))\n","\n","def sigd(x):\n","  return sig(x) * (1 - sig(x))\n","\n","#Fungsi Aktivasi Hyperbolic Tangent dengan turunannya\n","def tanh(x):\n","  return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n","\n","def tanh(x):\n","  return 1 - tanh(x)**2\n"],"metadata":{"id":"BTLa3NWvz7sq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def onehot_enc(lbl, min_val=0):\n","  mi = min(lbl)\n","  enc = np.full((len(lbl), max(lbl) - mi + 1), min_val, np.int8)\n","\n","  for i, x in enumerate(lbl):\n","    enc[i, x - mi] = 1\n","\n","  return enc\n","\n","def onehot_dec(enc, mi=0):\n","  return [np.argmax(e) + mi for e in enc]"],"metadata":{"id":"MopOydIkUjtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hihqFCY_ctZ3"},"source":["### a) Fungsi *Training* Backpropagation\n","\n","Tulis kode ke dalam *cell* di bawah ini:"]},{"cell_type":"code","metadata":{"id":"pTlk5igwcvc5"},"source":["def bp_fit_sig(X, target, layer_conf, max_epoch, max_error=.1, learn_rate=.1, print_per_epoch=100):\n","  start_time = time.time()\n","  np.random.seed(1)\n","  nin = [np.empty(i) for i in layer_conf]\n","  n = [np.empty(j + 1) if i < len(layer_conf) - 1 else\n","  np.empty(j) for i, j in enumerate(layer_conf)]\n","  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n","  dw = [np.empty((layer_conf[i] + 1, layer_conf[i + 1])) for i in range(len(layer_conf) - 1)]\n","  d = [np.empty(s) for s in layer_conf[1:]]\n","  din = [np.empty(s) for s in layer_conf[1:-1]]\n","  epoch = 0\n","  mse = 1\n","  for i in range(0, len(n)-1):\n","    n[i][-1] = 1\n","  while (max_epoch == -1 or epoch < max_epoch) and mse > max_error:\n","    epoch += 1\n","    mse = 0\n","    for r in range(len(X)):\n","      n[0][:-1] = X[r]\n","      for L in range(1, len(layer_conf)):\n","        nin[L] = np.dot(n[L-1], w[L-1])\n","        n[L][:len(nin[L])] = sig(nin[L])\n","      e = target[r] - n[-1]\n","      mse += sum(e ** 2)\n","      d[-1] = e * sigd(nin[-1])\n","      dw[-1] = learn_rate * d[-1] * n[-2].reshape((-1, 1))\n","      for L in range(len(layer_conf) - 1, 1, -1):\n","        din[L-2] = np.dot(d[L-1], np.transpose(w[L-1][:-1]))\n","        d[L-2] = din[L-2] * np.array(sigd(nin[L-1]))\n","        dw[L-2] = (learn_rate * d[L-2]) * n[L-2].reshape((-1, 1))\n","      w += dw\n","    mse /= len(X)\n","    if print_per_epoch > -1 and epoch % print_per_epoch == 0:\n","      print(f'Epoch {epoch}, MSE: {mse}')\n","  execution = time.time() - start_time\n","  print(\"Waktu eksekusi: %s detik\" % execution)\n","  return w, epoch, mse"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Membuat fungsi training backpropagation dengan menggunakan fungsi aktivasi tanh\n","def bp_fit_tanh(X, target, layer_conf, max_epoch, max_error=0.1, learn_rate=0.1, print_per_epoch=100):\n","    input_layer, hidden_layer, output_layer = layer_conf\n","    w_hidden = np.random.rand(input_layer, hidden_layer)\n","    w_output = np.random.rand(hidden_layer, output_layer)\n","\n","    for epoch in range(max_epoch):\n","        # Forward propagation\n","        hidden_input = np.dot(X, w_hidden)\n","        hidden_output = np.tanh(hidden_input)\n","        output_input = np.dot(hidden_output, w_output)\n","        output = np.tanh(output_input)\n","\n","        # Backpropagation\n","        error = target - output\n","        mse = np.mean(error**2)\n","        if mse < max_error:\n","            print(f\"Training completed after {epoch} epochs. MSE: {mse:.4f}\")\n","            break\n","\n","        # Update weights\n","        delta_output = error * (1 - output**2)\n","        delta_hidden = np.dot(delta_output, w_output.T) * (1 - hidden_output**2)\n","        w_output += learn_rate * np.dot(hidden_output.T, delta_output)\n","        w_hidden += learn_rate * np.dot(X.T, delta_hidden)\n","\n","        if epoch % print_per_epoch == 0:\n","            print(f\"Epoch {epoch}: MSE = {mse:.4f}\")\n","\n","    return [w_hidden, w_output, epoch]\n"],"metadata":{"id":"Pet6ptVOTxUR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJA_9btdc3ED"},"source":["### b) Fungsi *Testing* Backpropagation\n","\n","Tulis kode ke dalam *cell* di bawah ini:"]},{"cell_type":"code","metadata":{"id":"2zyXIu_ec9go"},"source":["def bp_predict_sig(X, w):\n","  n = [np.empty(len(i)) for i in w]\n","  nin = [np.empty(len(i[0])) for i in w]\n","  predict = []\n","  n.append(np.empty(len(w[-1][0])))\n","  for x in X:\n","    n[0][:-1] = x\n","    for L in range(0, len(w)):\n","      nin[L] = np.dot(n[L], w[L])\n","      n[L + 1][:len(nin[L])] = sig(nin[L])\n","    predict.append(n[-1].copy())\n","  return predict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZxy_M5Jc-ko"},"source":["### c) Klasifikasi dataset wine\n"]},{"cell_type":"markdown","source":["Lakukan pelatihan pada dataset wine dengan menggunakan 2 fungsi pelatihan yang telah dibuat!\n","\n","Konfigurasi kedua pelatihan harus sama (epoch, hidden layer, learning rate, dll).\n","Akurasi yang diharapkan di setiap pelatihan adalah > 0.98"],"metadata":{"id":"4xj7DqCdudcF"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"us5Kwtn5trf0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hw1L_Q3JdHk7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f849426-9fb4-4b4f-fe3d-ad777cf3afee","executionInfo":{"status":"ok","timestamp":1699889007525,"user_tz":-420,"elapsed":11912,"user":{"displayName":"LINTANG CAHYANING SUKMA","userId":"18146314551537415829"}}},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import minmax_scale\n","from sklearn.metrics import accuracy_score\n","\n","wine = datasets.load_wine()\n","X = minmax_scale(wine.data)\n","Y = onehot_enc(wine.target)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=.3,random_state=1)\n","#Isi jumlah layer yang digunakan dengan jumlah hidden layer #\n","w, ep, mse = bp_fit_sig(X_train, y_train, layer_conf=(13, 10, 3),\n","                        learn_rate=0.01, max_epoch=1000, max_error=0.01, print_per_epoch=25)\n","\n","print(f'Epochs: {ep}, MSE: {mse}')\n","\n","predict = bp_predict_sig(X_test, w)\n","predict = onehot_dec(predict)\n","y_test = onehot_dec(y_test)\n","accuracy = accuracy_score(predict, y_test)\n","\n","print('Output:', predict)\n","print('True :', y_test)\n","print('Accuracy:', accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-6557898fc4d8>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n","<ipython-input-5-6557898fc4d8>:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  w += dw\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25, MSE: 0.6566042896843435\n","Epoch 50, MSE: 0.6552151113995929\n","Epoch 75, MSE: 0.6536725397232533\n","Epoch 100, MSE: 0.6518550921653328\n","Epoch 125, MSE: 0.6495851428507287\n","Epoch 150, MSE: 0.646555389090612\n","Epoch 175, MSE: 0.6421712300340066\n","Epoch 200, MSE: 0.6351237271673849\n","Epoch 225, MSE: 0.6221232351325388\n","Epoch 250, MSE: 0.5952074234572872\n","Epoch 275, MSE: 0.5478830893834353\n","Epoch 300, MSE: 0.4914545797614302\n","Epoch 325, MSE: 0.4389756405689766\n","Epoch 350, MSE: 0.39521458262900006\n","Epoch 375, MSE: 0.35570668722227045\n","Epoch 400, MSE: 0.3159054441726655\n","Epoch 425, MSE: 0.2758060557419271\n","Epoch 450, MSE: 0.23836409068833195\n","Epoch 475, MSE: 0.20585969608975932\n","Epoch 500, MSE: 0.17895406452669071\n","Epoch 525, MSE: 0.15721440065319484\n","Epoch 550, MSE: 0.13974750666610194\n","Epoch 575, MSE: 0.1256249184029705\n","Epoch 600, MSE: 0.11406394103318941\n","Epoch 625, MSE: 0.104461044866247\n","Epoch 650, MSE: 0.09636764584618977\n","Epoch 675, MSE: 0.0894537930627171\n","Epoch 700, MSE: 0.08347597621939318\n","Epoch 725, MSE: 0.07825255550521389\n","Epoch 750, MSE: 0.07364609299628512\n","Epoch 775, MSE: 0.06955094761915483\n","Epoch 800, MSE: 0.0658846252330151\n","Epoch 825, MSE: 0.06258173195293235\n","Epoch 850, MSE: 0.059589711722647466\n","Epoch 875, MSE: 0.05686580415512473\n","Epoch 900, MSE: 0.054374840124318244\n","Epoch 925, MSE: 0.05208761741953061\n","Epoch 950, MSE: 0.04997968309839961\n","Epoch 975, MSE: 0.04803040551899603\n","Epoch 1000, MSE: 0.04622225643488231\n","Waktu eksekusi: 12.060701847076416 detik\n","Epochs: 1000, MSE: 0.04622225643488231\n","Output: [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0]\n","True : [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0]\n","Accuracy: 1.0\n"]}]},{"cell_type":"code","source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import minmax_scale\n","from sklearn.metrics import accuracy_score\n","\n","wine = datasets.load_wine()\n","X = minmax_scale(wine.data)\n","Y = onehot_enc(wine.target)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=.3,random_state=1)\n","#Isi jumlah layer yang digunakan dengan jumlah hidden layer #\n","result = bp_fit_tanh(X_train, y_train, layer_conf=(13, 10, 3), max_epoch=1000, max_error=0.01, learn_rate=0.004, print_per_epoch=25)\n","\n","w_hidden, w_output, epochs = result\n","\n","print(f'Epochs: {epochs}, MSE: {mse}')\n","\n","predict = bp_predict_tanh(X_test, (w_hidden, w_output))\n","predict = onehot_dec(predict)\n","y_test = onehot_dec(y_test)\n","accuracy = accuracy_score(predict, y_test)\n","\n","print('Output:', predict)\n","print('True :', y_test)\n","print('Accuracy:', accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mF18HdQgbCXy","outputId":"902f9c05-709a-40fa-8cb3-88d60cf3368c","executionInfo":{"status":"ok","timestamp":1699889286821,"user_tz":-420,"elapsed":940,"user":{"displayName":"LINTANG CAHYANING SUKMA","userId":"18146314551537415829"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: MSE = 0.6647\n","Epoch 25: MSE = 0.6596\n","Epoch 50: MSE = 0.5672\n","Epoch 75: MSE = 0.5933\n","Epoch 100: MSE = 0.6032\n","Epoch 125: MSE = 0.5324\n","Epoch 150: MSE = 0.5962\n","Epoch 175: MSE = 0.5330\n","Epoch 200: MSE = 0.3843\n","Epoch 225: MSE = 0.3672\n","Epoch 250: MSE = 0.2742\n","Epoch 275: MSE = 0.1329\n","Epoch 300: MSE = 0.1260\n","Epoch 325: MSE = 0.1034\n","Epoch 350: MSE = 0.1071\n","Epoch 375: MSE = 0.0922\n","Epoch 400: MSE = 0.0918\n","Epoch 425: MSE = 0.0823\n","Epoch 450: MSE = 0.0808\n","Epoch 475: MSE = 0.0735\n","Epoch 500: MSE = 0.0717\n","Epoch 525: MSE = 0.0661\n","Epoch 550: MSE = 0.0643\n","Epoch 575: MSE = 0.0601\n","Epoch 600: MSE = 0.0585\n","Epoch 625: MSE = 0.0553\n","Epoch 650: MSE = 0.0538\n","Epoch 675: MSE = 0.0513\n","Epoch 700: MSE = 0.0499\n","Epoch 725: MSE = 0.0479\n","Epoch 750: MSE = 0.0464\n","Epoch 775: MSE = 0.0447\n","Epoch 800: MSE = 0.0426\n","Epoch 825: MSE = 0.0424\n","Epoch 850: MSE = 0.0643\n","Epoch 875: MSE = 0.0568\n","Epoch 900: MSE = 0.0521\n","Epoch 925: MSE = 0.0496\n","Epoch 950: MSE = 0.0472\n","Epoch 975: MSE = 0.0457\n","Epochs: 999, MSE: 0.04622225643488231\n","Output: [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0]\n","True : [2, 1, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 1, 1, 2, 0, 1, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 1, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2, 0]\n","Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["# Pertanyaan"],"metadata":{"id":"Lp6y7VWfjVEX"}},{"cell_type":"markdown","source":["1.  Apa perbedaan dari penggunaan fungsi aktivasi sigmoid dengan fungsi aktivasi hyperbolic tangent?\n","2. Coba jelaskan alasan dari perbedaan tersebut sebisa kalian"],"metadata":{"id":"mP9dzq-kin0y"}},{"cell_type":"markdown","source":["# Jawaban"],"metadata":{"id":"RHEJApRcjXu7"}},{"cell_type":"markdown","source":["1.  Perbedaan\n","- Bentuk kurva\n","\n","Fungsi aktivasi sigmoid memiliki bentuk seperti sigmoid, sedangkan fungsi aktivasi hyperbolic tangent memiliki bentuk seperti kurva S.\n","\n","- Nilai output\n","\n","Fungsi aktivasi sigmoid memiliki nilai output yang mendekati 0 dan 1, sedangkan fungsi aktivasi hyperbolic tangent memiliki nilai output yang mendekati -1 dan 1.\n","\n","- Turunan\n","\n","Turunan fungsi aktivasi sigmoid adalah fungsi sigmoid itu sendiri, tetapi dikalikan dengan 1 - fungsi sigmoid. Turunan fungsi aktivasi hyperbolic tangent adalah fungsi hyperbolic tangent itu sendiri.\n","\n","- Kelebihan dan kekurangan\n","\n","Fungsi aktivasi sigmoid lebih mudah untuk dipelajari oleh jaringan saraf tiruan. Hal ini karena fungsi sigmoid memiliki turunan yang lebih sederhana. Namun, fungsi aktivasi sigmoid dapat menyebabkan overfitting pada data yang berukuran kecil.\n","\n","Fungsi aktivasi hyperbolic tangent lebih tahan terhadap overfitting. Hal ini karena fungsi aktivasi hyperbolic tangent memiliki nilai output yang lebih besar daripada fungsi aktivasi sigmoid.\n","\n","2.  Alasan\n","- Fungsi aktivasi sigmoid\n","\n","Fungsi aktivasi sigmoid memiliki bentuk kurva yang mirip dengan huruf S. Nilai output fungsi aktivasi sigmoid mendekati 0 dan 1. Turunan fungsi aktivasi sigmoid adalah fungsi sigmoid itu sendiri, tetapi dikalikan dengan 1 - fungsi sigmoid.\n","\n","Fungsi aktivasi sigmoid lebih mudah untuk dipelajari oleh jaringan saraf tiruan karena memiliki turunan yang lebih sederhana. Turunan fungsi aktivasi sigmoid dapat digunakan untuk menghitung gradient, yang digunakan untuk memperbarui bobot jaringan saraf tiruan selama proses training.\n","\n","Namun, fungsi aktivasi sigmoid dapat menyebabkan overfitting pada data yang berukuran kecil. Overfitting adalah kondisi di mana jaringan saraf tiruan terlalu cocok dengan data training, sehingga tidak dapat generalisasi dengan baik ke data baru.\n","\n","Hal ini disebabkan karena fungsi aktivasi sigmoid memiliki nilai output yang mendekati 0 dan 1. Nilai output yang mendekati 0 dan 1 dapat menyebabkan jaringan saraf tiruan menjadi terlalu sensitif terhadap data training.\n","\n","- Fungsi aktivasi hyperbolic tangent\n","\n","Fungsi aktivasi hyperbolic tangent memiliki bentuk kurva yang mirip dengan kurva S. Nilai output fungsi aktivasi hyperbolic tangent mendekati -1 dan 1. Turunan fungsi aktivasi hyperbolic tangent adalah fungsi hyperbolic tangent itu sendiri.\n","\n","Fungsi aktivasi hyperbolic tangent lebih tahan terhadap overfitting karena memiliki nilai output yang lebih besar daripada fungsi aktivasi sigmoid. Nilai output yang lebih besar dapat menyebabkan jaringan saraf tiruan menjadi kurang sensitif terhadap data training.\n","\n","Namun, fungsi aktivasi hyperbolic tangent lebih sulit untuk dipelajari oleh jaringan saraf tiruan karena memiliki turunan yang lebih kompleks. Turunan fungsi aktivasi hyperbolic tangent dapat digunakan untuk menghitung gradient, yang digunakan untuk memperbarui bobot jaringan saraf tiruan selama proses training."],"metadata":{"id":"4S55HVfLjaZ5"}}]}